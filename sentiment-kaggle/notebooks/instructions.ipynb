{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## UVU Data Science Club - Sentiment Demo\n",
    "\n",
    "This notebook is a demonstration of the deployment process of a simple sentiment classification model using `scikitlearn`. The model will be trained on 20NewsGroups data set available in scikit to train a NaiveBayes classifier for sentiment prediction. We'll also be using the `streamlit` package to deploy a frontend to our model to allow end users to submit text for classification and testing.\n",
    "\n",
    "The Notebook is divided in the following sections:\n",
    "\n",
    "1. Setup & Environment\n",
    "2. Load & Exploring the Data\n",
    "3. Feature Extraction (Bag of Words, Tokenization, Frequency Distribution)\n",
    "4. Training a Classifier\n",
    "5. Build Pipeline\n",
    "6. Performance Evaluation & Testing\n",
    "7. Deployment\n",
    "\n",
    "The code and instructions for this notebook are used from the [SciKitLearn Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). \n",
    "#### Citations\n",
    "@article{scikit-learn,\n",
    " title={Scikit-learn: Machine Learning in {P}ython},\n",
    " author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
    " journal={Journal of Machine Learning Research},\n",
    " volume={12},\n",
    " pages={2825--2830},\n",
    " year={2011}\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup & Environment\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "You should create a virtual environment to ensure dependency integrity. You can create a virtual environment inside this project directory by using your installed python interpreter using the instructions at the following [link](https://docs.python.org/3/tutorial/venv.html).\n",
    "\n",
    "Once you have your environment set up and activated. Download the required dependencies in this project by using the `python3 -m pip install -r requirements.txt` command at the root of your project using the `requirements.txt` file. Be sure to activate the virtual environment in your editor before running the notebook.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import dependencies for project\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Load Our Training Data\n",
    "BASE_TRAIN = \"../data/raw/train.tsv\"\n",
    "BASE_TEST = \"../data/raw/submission.tsv\"\n",
    "# labeled and submission data are 25000 rows long.\n",
    "labeled = pd.read_csv(BASE_TRAIN, header=0, delimiter=\"\\t\", quoting=3)\n",
    "prod = pd.read_csv(BASE_TEST, header=0, delimiter=\"\\t\", quoting=3)\n",
    "# local test size of data will be 80% of the labeled training set and local test data will be 20% in size\n",
    "train = labeled[0:20000]\n",
    "test = labeled[20000:25000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Inspect the structure of the data\n",
    "# Id - Unique identifier\n",
    "# Sentiment - Training Target, which means that's how we are going to train our data.\n",
    "# Review - The data that leads to our target.\n",
    "train.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We notice some dirty data so we should clean it.\n",
    "import re\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This function removes html symbols and the corresponding tags located within the tags in addition to repetitive backslashes.\n",
    "    \"\"\"\n",
    "    html_tags = re.compile('<.*?>')\n",
    "    clean = text.replace(\"\\\\\", \"\")\n",
    "    clean = clean.replace('\\'', \"\")\n",
    "    return re.sub(html_tags, '', clean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example of clean text\n",
    "print(f\"This is an example of how the text looks before being cleaned:\\n{train['review'].iloc[9]}\\n\\nCompared to after it gets cleaned:\\n\\n{clean_text(train['review'].iloc[9])}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lets apply this to our original data and create a new data frame with cleaned data and save it.\n",
    "train['review'] = train['review'].apply(clean_text)\n",
    "test['review'] = test['review'].apply(clean_text)\n",
    "prod['review'] = prod['review'].apply(clean_text)\n",
    "# save cleaned data to file\n",
    "train.to_csv(\"../data/processed/clean_local_train.csv\",index=False)\n",
    "test.to_csv(\"../data/processed/clean_local_test.csv\",index=False)\n",
    "prod.to_csv(\"../data/processed/prod.csv\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load clean data\n",
    "train = pd.read_csv(\"../data/processed/clean_local_train.csv\")\n",
    "test = pd.read_csv(\"../data/processed/clean_local_test.csv\")\n",
    "prod = pd.read_csv(\"../data/processed/prod.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"We want to ensure that our local training and test data is normally distributed for valid testing. The average sentiment for our training data is {train['sentiment'].mean()}. The average sentiment for our local testing data is {test['sentiment'].mean()}. This distribution of positive and negative is a confirmation we have a good training data set.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction: Bag of Words, Tokenization, and Frequency Distribution\n",
    "\n",
    "#### Bag of Words\n",
    "\n",
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "\n",
    "Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "\n",
    "For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "\n",
    "If n_samples == 10000, storing X as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = 4GB in RAM which is barely manageable on today’s computers.\n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "[SciKitLearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors:\n",
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices.\n",
    "\n",
    "CountVectorizer - It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. [GeeksForGeeks](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/#:~:text=CountVectorizer%20is%20a%20great%20tool,occurs%20in%20the%20entire%20text.&text=The%20value%20of%20each%20cell,in%20that%20particular%20text%20sample.)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# new instance of our CountVectorizer object we imported above\n",
    "count_vect = CountVectorizer()\n",
    "# create vectors of our cleaned data for analysis and traing.\n",
    "X_train_counts = count_vect.fit_transform(train['review'])\n",
    "# determine the size of our data. The x matches our rows above, the y are the features of vectors extracted. This includes a dictionary of features and n-grams.\n",
    "X_train_counts.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#tfidf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Classifer & Pipeline\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we supply the training data that we want the classifier to analyze and we provide a target of what the correct answer should be. These are indexed based so they line up 1:1\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train['sentiment'])\n",
    "# create a pipeline that's faster than the above steps separated.\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "# we can just pass the data in\n",
    "text_clf.fit(train['review'], train['sentiment'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Predictions\n",
    "predictions = text_clf.predict(test['review'])\n",
    "np.mean(predictions == test['sentiment'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save our model\n",
    "from joblib import dump, \n",
    "dump(text_clf, '../models/sentiment_classifier.joblib')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from joblib import  load\n",
    "clf = load('../models/sentiment_classifier.joblib')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}