{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## UVU Data Science Club - Sentiment Demo\n",
    "\n",
    "This notebook is a demonstration of the deployment process of a simple sentiment classification model using `scikitlearn`. The model will be trained on 20NewsGroups data set available in scikit to train a NaiveBayes classifier for sentiment prediction. We'll also be using the `streamlit` package to deploy a frontend to our model to allow end users to submit text for classification and testing.\n",
    "\n",
    "The Notebook is divided in the following sections:\n",
    "\n",
    "1. Setup & Environment\n",
    "2. Load & Exploring the Data\n",
    "3. Feature Extraction (Bag of Words, Tokenization, Frequency Distribution)\n",
    "4. Training a Classifier\n",
    "5. Build Pipeline\n",
    "6. Performance Evaluation & Testing\n",
    "7. Deployment\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup & Environment\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "You should create a virtual environment to ensure dependency integrity. You can create a virtual environment inside this project directory by using your installed python interpreter using the instructions at the following [link](https://docs.python.org/3/tutorial/venv.html).\n",
    "\n",
    "Once you have your environment set up and activated. Download the required dependencies in this project by using the `python3 -m pip install -r requirements.txt` command at the root of your project using the `requirements.txt` file. Be sure to activate the virtual environment in your editor before running the notebook.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# import dependencies for project\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "### Load Our Training Data\n",
    "BASE_TRAIN = \"../data/raw/train.tsv\"\n",
    "BASE_TEST = \"../data/raw/submission.tsv\"\n",
    "# labeled and submission data are 25000 rows long.\n",
    "labeled = pd.read_csv(BASE_TRAIN, header=0, delimiter=\"\\t\", quoting=3)\n",
    "prod = pd.read_csv(BASE_TEST, header=0, delimiter=\"\\t\", quoting=3)\n",
    "# local test size of data will be 80% of the labeled training set and local test data will be 20% in size\n",
    "train = labeled[0:20000]\n",
    "test = labeled[20000:25000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# Inspect the structure of the data\n",
    "# Id - Unique identifier\n",
    "# Sentiment - Training Target, which means that's how we are going to train our data.\n",
    "# Review - The data that leads to our target.\n",
    "train.head(20)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"2486_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What happens when an army of wetbacks, towelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"6811_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Although I generally do not like remakes beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"11744_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"7369_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I had a feeling that after \\\"Submerged\\\", thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"12081_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"note to George Litman, and others: the Myster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"3561_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Stephen King adaptation (scripted by King him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"4489_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"`The Matrix' was an exciting summer blockbust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"3951_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Ulli Lommel's 1980 film 'The Boogey Man' is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"3304_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This movie is one among the very few Indian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"9352_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Most people, especially young people, may not...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment                                             review\n",
       "0    \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1    \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2    \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3    \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4    \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5    \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
       "6    \"7166_2\"          0  \"This movie could have been very good, but com...\n",
       "7   \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
       "8     \"319_1\"          0  \"A friend of mine bought this film for £1, and...\n",
       "9   \"8713_10\"          1  \"<br /><br />This movie is full of references....\n",
       "10   \"2486_3\"          0  \"What happens when an army of wetbacks, towelh...\n",
       "11  \"6811_10\"          1  \"Although I generally do not like remakes beli...\n",
       "12  \"11744_9\"          1  \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...\n",
       "13   \"7369_1\"          0  \"I had a feeling that after \\\"Submerged\\\", thi...\n",
       "14  \"12081_1\"          0  \"note to George Litman, and others: the Myster...\n",
       "15   \"3561_4\"          0  \"Stephen King adaptation (scripted by King him...\n",
       "16   \"4489_1\"          0  \"`The Matrix' was an exciting summer blockbust...\n",
       "17   \"3951_2\"          0  \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18  \"3304_10\"          1  \"This movie is one among the very few Indian m...\n",
       "19  \"9352_10\"          1  \"Most people, especially young people, may not..."
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# We notice some dirty data so we should clean it.\n",
    "import re\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This function removes html symbols and the corresponding tags located within the tags in addition to repetitive backslashes.\n",
    "    \"\"\"\n",
    "    html_tags = re.compile('<.*?>')\n",
    "    clean = text.replace(\"\\\\\", \"\")\n",
    "    clean = clean.replace('\\'', \"\")\n",
    "    return re.sub(html_tags, '', clean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# example of clean text\n",
    "print(f\"This is an example of how the text looks before being cleaned:\\n{train['review'].iloc[9]}\\n\\nCompared to after it gets cleaned:\\n\\n{clean_text(train['review'].iloc[9])}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is an example of how the text looks before being cleaned:\n",
      "\"<br /><br />This movie is full of references. Like \\\"Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n",
      "\n",
      "Compared to after it gets cleaned:\n",
      "\n",
      "\"This movie is full of references. Like \"Mad Max II\", \"The wild one\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Lets apply this to our original data and create a new data frame with cleaned data and save it.\n",
    "train['review'] = train['review'].apply(clean_text)\n",
    "test['review'] = test['review'].apply(clean_text)\n",
    "prod['review'] = prod['review'].apply(clean_text)\n",
    "# save cleaned data to file\n",
    "train.to_csv(\"../data/processed/clean_local_train.csv\",index=False)\n",
    "test.to_csv(\"../data/processed/clean_local_test.csv\",index=False)\n",
    "prod.to_csv(\"../data/processed/prod.csv\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(f\"We want to ensure that our local training and test data is normally distributed for valid testing. The average sentiment for our training data is {train['sentiment'].mean()}. The average sentiment for our local testing data is {test['sentiment'].mean()}. This distribution of positive and negative is a confirmation we have a good training data set.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We want to ensure that our local training and test data is normally distributed for valid testing. The average sentiment for our training data is 0.4986. The average sentiment for our local testing data is 0.5056. This distribution of positive and negative is a confirmation we have a good training data set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction: Bag of Words, Tokenization, and Frequency Distribution\n",
    "\n",
    "#### Bag of Words\n",
    "\n",
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "\n",
    "Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "\n",
    "For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "\n",
    "If n_samples == 10000, storing X as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = 4GB in RAM which is barely manageable on today’s computers.\n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "[SciKitLearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors:\n",
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices.\n",
    "\n",
    "CountVectorizer - It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. [GeeksForGeeks](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/#:~:text=CountVectorizer%20is%20a%20great%20tool,occurs%20in%20the%20entire%20text.&text=The%20value%20of%20each%20cell,in%20that%20particular%20text%20sample.)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# new instance of our CountVectorizer object we imported above\n",
    "count_vect = CountVectorizer()\n",
    "# create vectors of our cleaned data for analysis and traing.\n",
    "X_train_counts = count_vect.fit_transform(train['review'])\n",
    "# determine the size of our data. The x matches our rows above, the y are the features of vectors extracted. This includes a dictionary of features and n-grams.\n",
    "X_train_counts.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20000, 73185)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "#tfidf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20000, 73185)"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Classifer & Pipeline\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# we supply the training data that we want the classifier to analyze and we provide a target of what the correct answer should be. These are indexed based so they line up 1:1\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train['sentiment'])\n",
    "# create a pipeline that's faster than the above steps separated.\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "# we can just pass the data in\n",
    "text_clf.fit(train['review'], train['sentiment'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "## Predictions\n",
    "predicted = text_clf.predict(test['review'])\n",
    "np.mean(predicted == test['sentiment'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('sentiment-demo': conda)"
  },
  "interpreter": {
   "hash": "6f87913ed6b8cfcaf6599801e51ac3ed58023d7e63890a026e4c46f24908edc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}